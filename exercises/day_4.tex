\documentclass[a4paper, 10pt]{scrartcl}

\usepackage{../generalstyle}
\usepackage{exercisestyle}


\title{Lineare Algebra II Repetitorium \\ Übungen, Tag 4}
\author{Jendrik Stelzner}
\date{\today}


\begin{document}
\maketitle


\begin{question}
  Es seien $V$ und $W$ zwei endlichdimensionale $K$-Vektorräume und $\beta \colon V \times V \to K$ und $\gamma \colon V \times V \to K$ zwei nicht-entartete symmetrische Bilinearformen.
  Es seien $\Phi_V \colon V \to V^*$, $v \mapsto \beta(-,v)$ und $\Phi_W \colon W \to W^*$, $w \mapsto \gamma(-,w)$.
  Zudem sei $f \colon V \to W$ ein $K$-lineare Abbildung und $f^T \colon W^* \to V^*$, $\psi \mapsto \psi \circ f$ die duale Abbildung.
  \begin{enumerate}[leftmargin=*]
    \item
      Zeigen Sie, dass eine Abbildung $f^* \colon W \to V$ das Diagramm
      \[
        \begin{tikzcd}[row sep = large, column sep = large, ampersand replacement=\&]
              W
              \arrow{r}{f^*}
              \arrow{d}[left]{\Phi_W}
          \&  V
              \arrow{d}{\Phi_V}
          \\
              W^*
              \arrow{r}{f^T}
          \&  V^*
        \end{tikzcd}
      \]
      genau dann zum kommutieren bringt, wenn
      \[
        \gamma(f(v), w) = \beta(v, f^*(w))
        \quad
        \text{für alle $v \in V$, $w \in W$}.
      \]
    \item
      Zeigen Sie, dass es eine eindeutige Abbildung $f^*$ gibt, die das obige Diagramm zum kommutieren bringt, und dass diese $K$-linear ist.
  \end{enumerate}
\end{question}


\begin{question}
  Es sei $V$ ein $K$-Vektorraum, $\beta \colon V \times V \to K$ eine symmetrische Bilinearform und $q \colon V \to K$, $v \mapsto \beta(v,v)$ die zugehörige quadratische Form.
  Zeigen Sie:
  \begin{enumerate}[leftmargin=*]
    \item
      Für $\ringchar(K) \neq 2$ ist
      \[
          \beta(v_1, v_2)
        = \frac{q(v_1 + v_2) - q(v_1) - q(v_2)}{2}
        \quad
        \text{für alle $v_1, v_2 \in V$}.
      \]
    \item
      Für $\ringchar(K) \neq 2$, $V \neq 0$ und $\beta$ nicht-entartet gibt es ein $v \in V$ mit $q(v) \neq 0$.
    \item
      Im Fall $\ringchar(K) = 2$ kann es verschiedene symmetrische Bilinearformen mit gleicher quadratischer Form geben.
      Geben Sie hierfür eine explizites Beispiel an.
  \end{enumerate}
\end{question}





\begin{question}
  Es seien
  \[
    A_1
    \coloneqq
    \begin{pmatrix}
      4 & 3 \\
      3 & 4
    \end{pmatrix},
    \,
    A_2
    \coloneqq
    \begin{pmatrix*}[r]
      3 &  4  \\
      4 & -3
    \end{pmatrix*},
    \,
    A_3
    \coloneqq
    \begin{pmatrix*}[r]
       3  & -2  &  0  \\
      -2  &  2  & -2  \\
       0  & -2  &  1
    \end{pmatrix*},
    \,
    A_4
    \coloneqq
    \begin{pmatrix*}[r]
       2  & -1  & 1 \\
      -1  &  2  & 1 \\
       1  &  1  & 2
    \end{pmatrix*}.
  \]
  \begin{enumerate}[leftmargin=*]
    \item
      Bestimmen Sie jeweils eine orthogonale Matrix $O_i \in \Orthogonal(n)$, so dass $O_i^T A_i O_i$ in Diagonalgestalt vorliegt.
    \item
      Entschieden Sie für die nicht-entarteten Fälle jeweils, ob es sich bei der Menge
      \[
        H_i \coloneqq \{ x \in \Reals^{n_i} \mid x^T A_i x = 10 \}
      \]
      um eine Ellipse oder eine Hyperbel bzw.\ um ein Ellipsoid oder ein einschaliges oder zweischaliges Hyperboloid handelt.
      Geben Sie jeweils die Länge der entsprechenden Hauptachsen an.
  \end{enumerate}
\end{question}


% \begin{solution}
%   Dritte Matrix: $T^3 - 6 T^2 + 3 T + 10$, Nullstellen $-1, 2, 5$.
%   Eigenvektoren:
%   \[
%     \cvector{1 \\ -2 \\ 2}
%     \cvector{-2 \\ -1 \\ 2}
%     \cvector{1 \\ 2 \\ 2}
%   \]
%   Vierte Matrix: $T^3 - 6 T^2 + 9 T = T(T-3)^2$.
%   Eigenvektoren (nicht orthogonal):
%   \[
%     \cvector{1 \\  0 \\ 1}
%     \cvector{1 \\ -1 \\ 0}
%     \cvector{1 \\  1 \\ 1}
%   \]
% 
% \end{solution}


\begin{question}
  Entscheiden Sie, welche der folgenden Aussagen für jeden reellen Vektorraum $V$ und jede symmetrische Bilinearform $\beta \colon V \times V \to \Reals$ mit $\beta \neq 0$ gilt.
  Geben Sie gegebenenfalls ein Gegenbeispiel.
  \begin{enumerate}[leftmargin=*]
    \item
      Ist $\beta(v, v) \geq 0$ für alle $v \in V$, so ist $\beta(-, -)$ ein Skalarprodukt.
    \item
      Ist $\basis{B} \subseteq V$ eine Basis von $V$ mit $\beta(v_1, v_2) > 0$ für alle $v_1, v_2 \in \basis{B}$, so ist $\beta(-, -)$ ein Skalarprodukt.
    \item
      Für jeden Untervektorraum $U \subseteq V$ gilt $U \subseteq (U^\perp)^\perp$.
    \item
      Die Teilmengen
      \[
        U_+ \coloneqq \{ v \in V \mid \beta(v, v) \geq 0 \}
        \quad\text{und}\quad
        U_- \coloneqq \{ v \in V \mid \beta(v, v) \leq 0 \}
      \]
      sind Untervektorräume von $V$.
    \item
      Für alle Untervektorräume $U_1, U_2 \subseteq V$ gilt $(U_1 + U_2)^\perp = U_1^\perp \cap U_2^\perp$.
    \item
      Die Teilmenge $U_0 \coloneqq \{ v \in V \mid \beta(v, v) = 0 \}$ ist ein Untervektorraum von $V$.
    \item
      Ist $\dim V < \infty$, so gilt $\dim V = \dim U + \dim U^\perp$ für jeden Untervektorraum $U \subseteq V$.
    \item
      Ist $\dim V < \infty$ und $U \subseteq V$ ein Untervektorraum mit $(U^\perp)^\perp = V$, so ist $U = V$.
  \end{enumerate}
\end{question}


\begin{solution}
  \begin{enumerate}[leftmargin=*]
    \item
      Die Aussage ist falsch.
      Man betrachte etwa $\beta \colon \Reals^2 \times \Reals^2 \to \Reals$ mit
      \[
                  \beta(v, w)
        \coloneqq v^T \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} w
        =         v_1 w_1
      \]
      Für diese gilt $\beta(v,v) = v_1^2 \geq 0$ für alle $v \in \Reals^2$, da $\beta(e_2, e_2) = 0$ ist $\beta$ aber nicht positiv definit.
    \item
      Die Aussage ist falsch.
      Man betrachte etwa $\beta \colon \Reals^2 \times \Reals^2 \to \Reals$ mit
      \[
                  \beta(v, w)
        \coloneqq v^T \begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix} w
        =         v_1 w_1 + 2 v_1 w_2 + 2 v_2 w_1 + v_2 w_2.
      \]
      Für die Standardbasis $\basis{B} \coloneqq \{e_1, e_2\}$ gilt zwar $\beta(e_i, e_j) \in \{1, 2\}$ für alle $i, j = 1, 2$, da $\beta(e_1 - e_2, e_1 - e_2) = -2 < 0$ ist $\beta$ aber nicht positiv definit.
      (Da die Determinante der obigen Matrix $-3$ ist, ergibt sich auch aus dem Hauptminorenkriterium, dass $\beta$ nicht positiv definit ist.)
    \item
      Die Aussage ist stimmt.
      Ist $u \in U$ und $v \in U^\perp$, so ist $\beta(u,v) = 0$ nach Definition von $U^\perp$, und somit $u \in (U^\perp)^\perp$.
    \item
      Die Aussage ist falsch.
      Betrachtet man etwa das vorherige Beispiel $\beta \colon \Reals^2 \to \Reals^2$ mit
      \[
                  \beta(v, w)
        \coloneqq v^T \begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix} w
        =         v_1 w_1 + 2 v_1 w_2 + 2 v_2 w_1 + v_2 w_2,
      \]
      so gilt $e_1, e_2 \in U_+$, da $\beta(e_1, e_1) = \beta(e_2, e_2) = 1 \geq 0$, da $\beta(e_1 - e_2, e_1 - e_2) = -2 < 0$ ist aber $e_1 - e_2 \notin U_+$.
    \item
      Die Aussage ist wahr:
      Da $U_1, U_2 \subseteq U_1 + U_2$ ist $(U_1 + U_2)^\perp \subseteq U_1^\perp, U_2^\perp$ und somit $(U_1 + U_2)^\perp \subseteq U_1^\perp \cap U_2^\perp$.
      Ist andererseits $v \in U_1^\perp \cap U_2^\perp$ und $u \in U_1 + U_2$, so gibt es $u_1 \in U_1$ und $u_2 \in U_2$ mit $u = u_1 + u_2$, weshalb
      \[
          \beta(v, u)
        = \beta(v, u_1 + u_2)
        = \underbrace{\beta(v, u_1)}_{= 0} + \underbrace{\beta(u, v_2)}_{=0}
        = 0.
      \]
      Also ist auch $U_1^\perp \cap U_2^\perp \subseteq (U_1 + U_2)^\perp$.
    \item
      Die Aussage ist falsch.
      Betrachtet man etwa $\beta \colon \Reals^2 \times \Reals^2 \to \Reals$ mit
      \[
                  \beta(v, w)
        \coloneqq v^T \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} w
        =         v_1 w_2 + v_2 w_1,
      \]
      so gilt $e_1, e_2 \in U_0$, aber $e_1 + e_2 \notin U_0$ da $\beta(e_1 + e_2, e_1 + e_2) = 2$.
    \item
      Die Aussage ist falsch.
      Betrachtet man etwa $\beta \colon \Reals^2 \times \Reals^2 \to \Reals$ mit
      \[
                  \beta(v, w)
        \coloneqq v^T \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} w
        =         v_1 w_1,
      \]
      so gilt für den eindimensionalen Untervektorraum $U = \Ell(e_2)$, dass $U^\perp = \Reals^2$, und somit
      \[
          \dim U + \dim U^\perp
        = 1 + 2
        = 3 > 2
        = \dim \Reals^2.
      \]
      (Man hat aber stets $\dim U + \dim U^\perp \geq \dim V$.)
    \item
      Die Aussage stimmt nicht.
      Betrachtet man etwa erneut $\beta \colon \Reals^2 \times \Reals^2 \to \Reals$ mit
      \[
                  \beta(v, w)
        \coloneqq v \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}
        =         v_1 w_1,
      \]
      so gilt für den echten Untervektorraum $U \coloneqq \Ell(e_1) \subsetneq \Reals^2$, dass $U^\perp = \Ell(e_2)$ und somit $(U^\perp)^\perp = \Reals^2$.
  \end{enumerate}
\end{solution}



\begin{question}
  Es sei $n \geq 1$.
  Es sei
  \[
    S_+ \coloneqq \{A \in \Mat_n(\Reals) \mid A^T = A\}
  \]
  der Vektorraum der symmetrischen reellen Matrizen und
  \[
    S_- \coloneqq \{A \in \Mat_n(\Reals) \mid A^T = -A\}
  \]
  der Vektorraum der schiefsymmetrischen reellen Matrizen.
  \begin{enumerate}[leftmargin=*]
    \item
      Zeigen Sie, dass $\tr(AB) = \tr(BA)$ für alle $A, B \in \Mat_n(\Reals)$.
    \item
      Zeigen Sie, dass $\sigma \colon \Mat_n(\Reals) \times \Mat_n(\Reals) \to \Reals$ mit
      \[
        \sigma(A,B) \coloneqq \tr(AB)
        \quad
        \text{für alle $A, B \in \Mat_n(\Reals)$}
      \]
      eine symmetrische Bilinearfom ist.
    \item
      Zeigen Sie, dass $\Mat_n(\Reals) = S_+ \oplus S_-$.
    \item
      Zeigen Sie, dass $S_+$ und $S_-$ orthogonal zueinander bezüglich $\sigma$ sind.
    \item
      Zeigen Sie, dass die Einschränkung $\sigma|_{S_+ \times S_+}$ positiv definit ist, und dass die Einschränkung $\sigma|_{S_- \times S_-}$ negativ definit.
    \item
      Bestimmen Sie eine Basis $\basis{C}$ von $\Mat_2(\Reals)$, so dass $\Mat_\basis{C}(\sigma)$ in Diagonalgestalt ist und $1, -1, 0$ die einzigen möglichen Diagonaleinträge sind.
  \end{enumerate}
\end{question}


\begin{solution}
  \begin{enumerate}[leftmargin=*]
    \item
      Es ist
      \[
          \tr(AB)
        = \sum_{i=1}^n (AB)_{ii}
        = \sum_{i=1}^n \sum_{j=1}^n A_{ij} B_{ji}
        = \sum_{j=1}^n \sum_{i=1}^n B_{ji} A_{ij}
        = \sum_{j=1}^n (BA)_{jj}
        = \tr(BA).
      \]
    \item
      Die Bilinearität von $\sigma$ ergibt sich durch direktes Nachrechnen, wobei man nutzt, dass die Matrixmultiplikation $\Mat_n(\Reals) \times \Mat_n(\Reals) \to \Mat_n(\Reals)$, $(A_1, A_2) \mapsto A_1 A_2$ bilinear ist, und dass die Spur $\tr \colon \Mat_n(\Reals) \to \Reals$ linear ist.
      Nach dem ersten Aufgabenteil ist $\sigma$ symmetrisch.
    \item
      Für $A \in S_+ \cap S_-$ ist $A = A^T = -A$ und somit $A = 0$, und jede Matrix $A \in \Mat_n(\Reals)$ lässt sich als
      \[
        A = \frac{A + A^T}{2} + \frac{A - A^T}{2}
      \]
      schreiben, wobei der erste Summand symmetrisch ist und der zweite schiefsymmetrisch.
      
      Alternativ lässt sich der Endomorphismus $s \colon \Mat_n(\Reals) \to \Mat_n(\Reals)$ mit $s(A) = (A + A^T)/2$ betrachten.
      Dieser ist idempotent, d.h.\ es gilt $s^2 = s$, weshalb $\Mat_n(\Reals) = \im(e) \oplus \ker(e)$.
      Mit $\im(e) = S_+$ und $\ker(e) = S_-$ ergibt sich die Zerlegung.
    \item
      Ist $A_+ \in S_+$ und $A_- \in S_-$, so ist
      \begin{align*}
             \sigma(A_+, A_-)
        &=   \tr( A_+ A_- )
         =   \tr( (A_+ A_-)^T )
         =   \tr( A_-^T A_+^T ) \\
        &=   \tr( -(A_-) A_+ )
         =  -\tr( A_- A_+)
         =  -\sigma(A_-, A_+)
         =  -\sigma(A_+, A_-)
      \end{align*}
      und somit $\sigma(A_+, A_-) = 0$.
    \item
      Für alle $A \in S_+$ ist
      \[
              \sigma(A, A)
        =     \tr(A A)
        =     \tr(A A^T)
        =     \sum_{i=1}^n (A A^T)_{ii}
        =     \sum_{i=1}^n \sum_{j=1}^n A_{ij} A^T_{ji}
        =     \sum_{i,j = 1, \dotsc, n} A_{ij}^2
        \geq  0
      \]
      und Gleichheit gilt genau dann, wenn $A_{ij} = 0$ für alle $i,j = 1, \dotsc, n$, wenn also $A = 0$.
      
      Ist andererseits $A \in S_-$, so ergibt sich analog zur obigen Rechnung, dass
      \[
                \sigma(A, A)
        =     - \sum_{i, j = 1, \dotsc, n} A_{ij}^2
        \leq    0,
      \]
      und Gleichheit gilt genau dann, wenn $A = 0$.
    \item
      Da $\sigma|_{S_+ \times S_+}$ positiv definit und $\sigma|_{S_- \times S_-}$ negativ definit ist, wissen wir nach dem Sylvesterschen Trägheitssatz bereits, dass der Diagonaleintrag $1$ mit Vielfachheit $\dim(S_+) = 3$ auftreten wird, und der Diagonaleintrag $-1$ mit Vielfachheit $1$.
      
      Da die Summe $\Mat_n(\Reals) = S_+ \oplus S_-$ orthogonal ist, genügt es für jeden dieser beiden Untervektorräume eine entsprechende Basis bezüglich der Einschränkung $\sigma|_{S_+ \times S_+}$, bzw.\ $\sigma|_{S_- \times S_-}$ zu finden, und diese anschließend zusammenzuführen.
      
      Für den eindimensionalen Raum $S_-$ beginnen wir mit dem Basisvektor
      \[
        \tilde{A}_1
        \coloneqq
        \begin{pmatrix*}[r]
           0  & 1 \\
          -1  & 0
        \end{pmatrix*}.
      \]
      Für diesen gilt $\sigma(\tilde{A}_1, \tilde{A}_1) = -2$, weshalb wir $\tilde{A}_1$ zu
      \[
        A_1
        \coloneqq
        \frac{1}{\sqrt{2}}
        \tilde{A}_1
        =
        \frac{1}{\sqrt{2}}
        \begin{pmatrix*}[r]
           0  & 1 \\
          -1  & 0
        \end{pmatrix*}
      \]
      normieren.
      
      Für $S_+$ beginnen wir mit den drei Basisvektoren
      \[
        \tilde{A}_2
        \coloneqq
        \begin{pmatrix}
          1 & 0 \\
          0 & 0
        \end{pmatrix},
        \quad
        \tilde{A}_3
        \coloneqq
        \begin{pmatrix}
          0 & 0 \\
          0 & 1
        \end{pmatrix},
        \quad
        \tilde{A}_4
        \coloneqq
        \begin{pmatrix}
          0 & 1 \\
          1 & 0
        \end{pmatrix}.
      \]
      Da $\sigma_{S_+ \times S_+}$ positiv definit, also ein Skalarprodukt ist, können wir nun das Gram-Schmidt-Verfahren anwenden.
      (Zur einfacheren Berechnung von $\sigma|_{S_+ \times S_+}$ kann man sich zunächst überlegen, dass $\sigma(A, B) = \sum_{i,j} A_{ij} B_{ij}$ für alle $A, B \in S_+$.)
      Damit erhalten wir für $S_+$ die Basis
      \[
        A_2
        \coloneqq
        \begin{pmatrix}
          1 & 0 \\
          0 & 0
        \end{pmatrix},
        \quad
        A_3
        \coloneqq
        \begin{pmatrix}
          0 & 0 \\
          0 & 1
        \end{pmatrix},
        \quad
        A_4
        \coloneqq
        \frac{1}{\sqrt{2}}
        \begin{pmatrix}
          0 & 1 \\
          1 & 0
        \end{pmatrix}.
      \]
      
      Insgesamt erhalten wir somit eine Basis $\basis{C} = (A_1, A_2, A_3, A_4)$ von $\Mat_n(\Reals)$ mit
      \[
        \Mat_{\basis{C}}(\sigma)
        =
        \begin{pmatrix}
          -1  &   &   &   \\
              & 1 &   &   \\
              &   & 1 &   \\
              &   &   & 1
        \end{pmatrix}.
      \]
  \end{enumerate}
\end{solution}


\begin{question}
  Es sei $V$ ein $K$-Vektorraum mit Basis $\basis{C} = (v_1, \dotsc, v_n)$, $\basis{C}^* = (v_1^*, \dotsc, v_n^*)$ die duale Basis von $V^*$ und $\beta \colon V \times V \to K$ eine Bilinearform.
  \begin{enumerate}[leftmargin=*]
    \item
      Zeigen Sie für die lineare Abbildung $\Phi \colon V \to V^*$, $v \mapsto \beta(-,v)$, dass
      \[
        \Mat_{\basis{C}, \basis{C}^*}(\Phi)_{ij} = \beta(v_i, v_j)
        \quad
        \text{für alle $i, j = 1, \dotsc, n$}.
      \]
    \item
      Es sei $\Psi \colon V \to K^n$ der eindeutige Isomorphismus mit
      \[
          \Psi(\lambda_1 v_1 + \dotsb + \lambda_n v_n)
        = \cvector{\lambda_1 \\ \vdots \\ \lambda_n}
        \quad
        \text{für alle $\lambda_1, \dotsc, \lambda_n \in \Reals$},
      \]
      dass $\Mat_\basis{C}(\beta)$ eindeutig dadurch bestimmt ist, dass
      \[
        \beta(v, w) = \Psi(v)^T \, \Mat_\basis{C}(\beta) \, \Psi(w)
        \quad
        \text{für alle $v, w \in V$}.
      \]
    \item
      Folgern Sie, dass $\beta$ genau dann symmetrisch ist, wenn $\Mat_\basis{C}(\beta)$ symmetrisch ist.
  \end{enumerate}
\end{question}


\begin{solution}
  \begin{enumerate}[leftmargin=*]
    \item
      Ist $A \coloneqq \Mat_{\basis{C}, \basis{C}^*}(\Phi)$, so ist $\Phi(v_j) = \sum_{i=1}^n A_{ij} v_i^*$ für alle $j = 1, \dotsc, n$.
      Deshalb ist zum einen
      \begin{gather*}
          \Phi(v_j)(v_i)
        = \beta(-, v_j)(v_i)
        = \beta(v_i, v_j)
      \shortintertext{und zum anderen}
          \Phi(v_j)(v_i)
        = \sum_{k=1}^n A_{kj} v_k^*(v_i)
        = \sum_{k=1}^n A_{kj} \delta_{ki}
        = A_{ij},
      \end{gather*}
      und somit $A_{ij} = \beta(v_i, v_j)$ for all $i, j = 1, \dotsc, n$.
    \item
      Sind $v, w \in V$ mit $v = \sum_{i=1}^n \lambda_i v_i$ und $w = \sum_{i=1}^n \mu_i v_i$, so ist
      \[
          \beta(v, w)
        = \beta\left( \sum_{i=1}^n \lambda_i v_i, \sum_{j=1}^n \mu_j v_j \right)
        = \sum_{i,j = 1}^n \lambda_i \mu_j \beta(v_i, v_j).
      \]
      Für $A \in \Mat_n(\Reals)$ gilt andererseits
      \[
          \Phi(v)^T A \, \Phi(w)
        = \cvector{\lambda_1 \\ \vdots \\ \lambda_n}^T A \cvector{\mu_1 \\ \vdots \\ \mu_n}
        = \sum_{i,j = 1, \dotsc, n} \lambda_i \mu_j A_{ij}.
      \]
      Somit ist $\beta(v, w) = \Phi(v)^T A \Phi(w)$ für alle $v, w \in V$ äquivalent dazu, dass $A_{ij} = \beta(v_i, v_j)$ für alle $i, j = 1, \dotsc, n$, was nach dem vorherigen Aufgabenteil äquivalent zu $A = \Mat_\basis{C}(\Phi)$ ist.
      (Man beachte für die Eindeutigkeit, dass
      \[
          A_{ij}
        = e_i A e_j
        = \Phi(v_i)^T A \Phi(v_j)
        = \beta(v_i, v_j)
      \]
      gelten muss.)
    \item
      Da $\basis{C}$ eine Basis von $V$ ist, ist $\beta$ genau dann symmetrisch, wenn $\beta(v_i, v_j) = \beta(v_j, v_i)$ for alle $i, j = 1, \dotsc, n$.
      Wegen $\Mat_\basis{C}(\Phi)_{ij} = \beta(v_i, v_j)$ für alle $i, j = 1, \dotsc, n$ ist dies genau dann der Fall, wenn $\Mat_{\basis{C}}(\Phi)_{ij} = \Mat_{\basis{C}}(\Phi)_{ji}$ für alle $i, j = 1, \dotsc, n$, wenn also $\Mat_{\basis{C}}(\Phi)$ symmetrisch ist.
  \end{enumerate}
\end{solution}















\newpage


\printsolutions



\end{document}
